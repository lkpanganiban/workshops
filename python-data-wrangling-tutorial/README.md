# Web Scraping and Data Wrangling with Python

The following repository is an introduction to web scraping and data wrangling. This will go over the basic workflow of web scraping and data wrangling.

## General Usage and Setup
1. This uses `Jupyter Notebook` as the main tool to execute the various notebooks. This requires that the participant will install the necessary requirements in their own machine. This repository contains a `requirements.txt` file where the dependencies are listed.
2. The participant may opt to use [Google Colab](https://colab.research.google.com/) to run this notebook (this requires a google account). The `Google Colab` already contains the necessary tools and environments to run this exercise. 
   - In the `Google Colab` popup, select **Github** and paste the link of this repository. If this popup is not present, select **File** and select **Upload notebook**.
   - Click **Copy to Drive**, this will create a **Colab folder** in your Google Drive.

## Libraries
The following libraries were used to run these exercises.
- Beautifulsoup4
- Pandas
- Matplotlib

## Exercises
This repository contains the notebooks used for web scraping and data wrangling sessions. 

### Python 101
The `Python 101` notebook that provides crash course to the participant about Python as a programming language.

### Web Scraping
The `Web Scraping` notebook is a tutorial that focuses on the workflow of web scraping using `beautifulsoup4` and `requests` libraries. This targets a `Wikipedia` page to scrape, extract and store the data into a CSV file.

### Data Wrangling
The `Data Wrangling` notebook provides a workflow in transforming data into various formats and also a way to visualize these data. The main libraries to be used here is `pandas` and `matplotlib`. 