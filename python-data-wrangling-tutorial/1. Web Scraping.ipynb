{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb112202",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "**Welcome to the Web Scraping Notebook!**\n",
    "\n",
    "The notebook showcases the power of Python to scrape data from the internet. We will use the library `beautifulsoup4` to do the following session and exercises.\n",
    "\n",
    "**Note: This is not a definitive guide.**\n",
    "\n",
    "## What is Web Scraping?\n",
    "Due to the abundance of information being generated in the internet today, access to these information have been relatively easy. The main problem arises on the collection of these data in bulk, organizing and analyzing. \n",
    "\n",
    "Web scraping is a method used by most organizations to gather data in bulk. Web scraping automatically extracts data and presents in a chosen format e.g. CSV, XLSX.\n",
    "\n",
    "### Web Scraping and AI\n",
    "Machine learning models require a lot of data, web or data scraping provides a way to generate and aggregate these data to be fed to create a machine learning model. As an example, `ChatGPT` was built by scraping through the entire internet. \n",
    "\n",
    "## Prerequisites\n",
    "Before we start, you will need to have a basic knowledge of the following technologies\n",
    "\n",
    "- [HTML](https://www.w3schools.com/html/default.asp)\n",
    "- [Python](https://www.python.org/)\n",
    "\n",
    "## Primary Tool\n",
    "The primary library or tool that we will be using is `Beautifulsoup4`. This library allows us to parse through `HTML` and retrieve the value that we want. There are also a number of scraping tools and libraries supported by Python like `scrapy` which act like as a scraping framework.\n",
    "\n",
    "## Anatomy of a Web Scraper\n",
    "Any web scraper has a basic anatomy. It requires a retriever, parser and a transformer.\n",
    "\n",
    "### Retriever\n",
    "Retrievers are responsible to fetch the information from a website. Retrievers are not responsible in extracting or parsing the information. When a retriever fetches a website, it will return a blob of HTML tags where it will be passed to the parser/extractor. \n",
    "\n",
    "The logic of the retriever is only limited to generating the URL to be retrieved e.g. passing arguments or authentication parameters.  \n",
    "\n",
    "### Parser/Extractor\n",
    "Parsers are responsible to go through the information fetched by the retriever. Logic controls like extract all tables or images will be placed here. Once the information or value is extracted, it will be then passed to the transfomer which changes the form or structure. \n",
    "\n",
    "### Transfomer\n",
    "Transformers are responsible to change the form or structure of the value.\n",
    "\n",
    "### Storer\n",
    "The changed value can be stored into a database or a file like a CSV. Python has a wide ecosystem of connectors to databases and libraries to generate custom formats.\n",
    "\n",
    "## Hands-on\n",
    "For this tutorial, we will be scraping the following [link](https://en.wikipedia.org/wiki/Economy_of_the_Philippines). In this page, we will be extracting the **Regional Accounts** table. Let us try to inspect this page and try to understand its structure. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "147e51a3",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "We will import the `requests` and `bs4` libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa50e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f104843e",
   "metadata": {},
   "source": [
    "### Retriever\n",
    "To implement a retriever step, we will be using the `requests` library. The requests library allows us to fetch the contents of a website. The content that is retrieved is in a plain text format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e9dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "url = requests.get('https://en.wikipedia.org/wiki/Economy_of_the_Philippines')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e04c94ea",
   "metadata": {},
   "source": [
    "### Parser\n",
    "Once we have retrieved the contents in plain text, we will be using the library called `beautifulsoup4` or `bs4`. This library already has some functions which abstracts the parsing and filtering of an HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeed909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = BeautifulSoup(url.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3416921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all the tables inside this webpage\n",
    "all_tables = source.find_all('table', class_=\"wikitable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2451da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the regional table\n",
    "regional_table = all_tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f988608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Region',\n",
       " 'GRDP(PHP, thousands)',\n",
       " 'Agriculture(PHP, thousands)',\n",
       " 'Industry(PHP, thousands)',\n",
       " 'Services(PHP, thousands)',\n",
       " 'GRDP per capita(PHP)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the top headers\n",
    "count = 0\n",
    "attribute_headers = []\n",
    "rows = regional_table.find_all('tr')\n",
    "for row in rows:\n",
    "    headers = row.find_all('th')\n",
    "    if headers is None: continue\n",
    "    if count == 0: attribute_headers = [header.text.replace(\"\\n\", \"\") for header in headers] # \n",
    "    break\n",
    "attribute_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26088f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the column values\n",
    "data = {}\n",
    "rows = regional_table.find_all('tr')\n",
    "for row in rows:\n",
    "    headers = row.find('th')\n",
    "    if headers is None: continue\n",
    "    columns = row.find_all('td')\n",
    "    for idx, col in enumerate(columns):\n",
    "        if idx % 2 != 0: continue\n",
    "        city = headers.text.replace(\"\\n\", \"\")\n",
    "        value = float(col.text.replace(\"\\n\", \"\").replace(\",\", \"\"))\n",
    "        if city in data: data[city].append(value)\n",
    "        else: data[city] = [value]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f62db1",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "Finally, we will implement some post-processing and transform and combine the extracted header attributes and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d983df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = [attribute_headers]\n",
    "for d in data:\n",
    "    tmp_list = [d] + data[d]\n",
    "    csv_data.append(tmp_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3c255",
   "metadata": {},
   "source": [
    "### Storer\n",
    "We will then store the transformed data into a CSV. Python has the `csv` module as a part of its standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c93e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "with open(\"data/wiki_economics.csv\", \"w\") as econ_csv:\n",
    "    data_writer = csv.writer(econ_csv, delimiter=\",\")\n",
    "    data_writer.writerows(csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd70b2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully scraped the web page. To become proficient in web scraping, it will require hundreds of hours of practice and exposure to various structures. In the next session, we will be using this data to clean and visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e543f66",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
